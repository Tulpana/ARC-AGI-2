{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28546592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T02:59:41.409659Z",
     "iopub.status.busy": "2025-10-28T02:59:41.409302Z",
     "iopub.status.idle": "2025-10-28T02:59:41.759064Z",
     "shell.execute_reply": "2025-10-28T02:59:41.757878Z"
    },
    "papermill": {
     "duration": 0.356174,
     "end_time": "2025-10-28T02:59:41.760811",
     "exception": false,
     "start_time": "2025-10-28T02:59:41.404637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYNC] Copying /kaggle/input/arc-agi-2-dataset-1027/arc_agi_2_submission -> /kaggle/working/arc_agi_2_submission\n",
      "[SPEC] schema not mounted (lightweight validation will be used).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Locate package + (optional) schema (safe) ===\n",
    "from pathlib import Path\n",
    "import os, sys, shutil\n",
    "\n",
    "NOTEBOOK_ROOT = Path.cwd().resolve()\n",
    "WORK_ROOT = Path(\"/kaggle/working\") if Path(\"/kaggle/working\").exists() else NOTEBOOK_ROOT\n",
    "PACKAGE_NAME = \"arc_agi_2_submission\"\n",
    "\n",
    "def _iter_pkg(base: Path):\n",
    "    if not base or not base.exists(): return\n",
    "    base = base.resolve()\n",
    "    if base.name == PACKAGE_NAME and (base / \"kaggle_entry.py\").exists():\n",
    "        yield base\n",
    "    for d in base.iterdir():\n",
    "        if d.is_dir():\n",
    "            if d.name == PACKAGE_NAME and (d / \"kaggle_entry.py\").exists():\n",
    "                yield d.resolve()\n",
    "            cand = d / PACKAGE_NAME\n",
    "            if cand.exists() and (cand / \"kaggle_entry.py\").exists():\n",
    "                yield cand.resolve()\n",
    "\n",
    "# 1) find source package (prefer /kaggle/input)\n",
    "search_roots = [Path(os.getenv(\"ARC_SUBMISSION_ROOT\") or \"\")] + [\n",
    "    Path(\"/kaggle/input\"), NOTEBOOK_ROOT, NOTEBOOK_ROOT.parent, WORK_ROOT\n",
    "]\n",
    "submission_src = None\n",
    "seen = set()\n",
    "for root in filter(lambda p: p and p.exists(), search_roots):\n",
    "    for cand in _iter_pkg(root):\n",
    "        if cand in seen: continue\n",
    "        seen.add(cand)\n",
    "        if (cand / \"predict_competition.py\").exists():\n",
    "            submission_src = cand; break\n",
    "    if submission_src: break\n",
    "\n",
    "if not submission_src:\n",
    "    raise FileNotFoundError(\"Attach the arc_agi_2_submission dataset or set ARC_SUBMISSION_ROOT.\")\n",
    "\n",
    "submission_dst = (WORK_ROOT / PACKAGE_NAME).resolve()\n",
    "if submission_src != submission_dst:\n",
    "    if submission_dst.exists():\n",
    "        print(f\"[SYNC] Clearing {submission_dst}\")\n",
    "        shutil.rmtree(submission_dst)\n",
    "    print(f\"[SYNC] Copying {submission_src} -> {submission_dst}\")\n",
    "    shutil.copytree(submission_src, submission_dst)\n",
    "else:\n",
    "    print(f\"[SYNC] Package already in working dir: {submission_dst}\")\n",
    "\n",
    "if str(submission_dst) not in sys.path:\n",
    "    sys.path.insert(0, str(submission_dst))\n",
    "\n",
    "# Optional: try to find schema, but never hard-fail\n",
    "def _find_schema():\n",
    "    names = [\"arc-prize-2025\", \"ARC Prize 2025\"]\n",
    "    for name in names:\n",
    "        for base in [submission_dst.parent, NOTEBOOK_ROOT, NOTEBOOK_ROOT.parent, Path(\"/kaggle/input\")]:\n",
    "            p = base / name\n",
    "            if p.exists() and (p / \"submission.schema.json\").exists():\n",
    "                return p.resolve()\n",
    "    # last resort: search\n",
    "    try:\n",
    "        hit = next(Path(\"/kaggle/input\").rglob(\"submission.schema.json\"), None)\n",
    "        return hit.parent.resolve() if hit else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "spec_root = _find_schema()\n",
    "print(f\"[SPEC] {spec_root}\" if spec_root else \"[SPEC] schema not mounted (lightweight validation will be used).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7823bdab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T02:59:41.767657Z",
     "iopub.status.busy": "2025-10-28T02:59:41.767350Z",
     "iopub.status.idle": "2025-10-28T02:59:42.043127Z",
     "shell.execute_reply": "2025-10-28T02:59:42.041754Z"
    },
    "papermill": {
     "duration": 0.281395,
     "end_time": "2025-10-28T02:59:42.045033",
     "exception": false,
     "start_time": "2025-10-28T02:59:41.763638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DISCOVER] Selected RUN_SET=test from /kaggle/input/arc-prize-2025/arc-agi_test_challenges.json â†’ 240 tasks\n",
      "[CURATE] ARC_DATA_DIR -> /kaggle/working/arc_curated_dataset (contains only arc-agi_test_challenges.json)\n",
      "[PATCH] load_aggregated_tasks() overridden to return curated 'selected' only.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Select RUN_SET, curate dataset dir with ONE file, override loader ===\n",
    "from pathlib import Path\n",
    "import os, json, shutil\n",
    "\n",
    "# Pick what to run: \"test\" (submission), \"evaluation\", \"training\"\n",
    "RUN_SET = os.getenv(\"ARC_RUN_SET\", \"test\").lower()\n",
    "CANON = {\n",
    "    \"test\":       \"arc-agi_test_challenges.json\",\n",
    "    \"evaluation\": \"arc-agi_evaluation_challenges.json\",\n",
    "    \"training\":   \"arc-agi_training_challenges.json\",\n",
    "}\n",
    "if RUN_SET not in CANON:\n",
    "    print(f\"[WARN] Unknown ARC_RUN_SET={RUN_SET!r}; defaulting to 'test'\")\n",
    "    RUN_SET = \"test\"\n",
    "\n",
    "# Find a source root that has the chosen file (accept hyphen/underscore variants)\n",
    "SOURCE_ROOTS = [Path(\"/kaggle/input\"), submission_dst.parent, NOTEBOOK_ROOT]\n",
    "VARIANTS = {\n",
    "    \"test\":       [\"arc-agi_test_challenges.json\", \"arc-agi_test-challenges.json\"],\n",
    "    \"evaluation\": [\"arc-agi_evaluation_challenges.json\", \"arc-agi_evaluation-challenges.json\"],\n",
    "    \"training\":   [\"arc-agi_training_challenges.json\", \"arc-agi_training-challenges.json\"],\n",
    "}\n",
    "\n",
    "def _find_source_file():\n",
    "    for base in SOURCE_ROOTS:\n",
    "        if not base.exists(): continue\n",
    "        # search shallow and one directory down\n",
    "        for d in [base] + [p for p in base.iterdir() if p.is_dir()]:\n",
    "            for name in VARIANTS[RUN_SET]:\n",
    "                p = d / name\n",
    "                if p.exists():\n",
    "                    return p.resolve()\n",
    "    return None\n",
    "\n",
    "src_file = _find_source_file()\n",
    "if not src_file:\n",
    "    raise SystemExit(\n",
    "        \"Could not find the chosen split.\\n\"\n",
    "        f\"RUN_SET={RUN_SET}\\n\"\n",
    "        \"Looked for any of:\\n  - \" + \"\\n  - \".join(VARIANTS[RUN_SET]) + \"\\n\"\n",
    "        \"Attach a dataset such as arc-prize-2025 or arc-agi-2-public-dataset.\"\n",
    "    )\n",
    "\n",
    "# Read tasks into memory (normalize dict form)\n",
    "with open(src_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    blob = json.load(f)\n",
    "if isinstance(blob, dict):\n",
    "    selected = {str(k): v for k, v in blob.items()}\n",
    "else:\n",
    "    selected = {}\n",
    "    for item in blob:\n",
    "        if isinstance(item, dict):\n",
    "            tid = item.get(\"task_id\") or item.get(\"id\")\n",
    "            task_obj = item.get(\"task\") or item\n",
    "            if tid and isinstance(task_obj, dict):\n",
    "                selected[str(tid)] = task_obj\n",
    "\n",
    "print(f\"[DISCOVER] Selected RUN_SET={RUN_SET} from {src_file} â†’ {len(selected)} tasks\")\n",
    "\n",
    "# Curate: write ONE file in a fresh directory so the library cannot aggregate others\n",
    "curated = Path(\"/kaggle/working/arc_curated_dataset\")\n",
    "if curated.exists(): shutil.rmtree(curated)\n",
    "curated.mkdir(parents=True, exist_ok=True)\n",
    "out_name = CANON[RUN_SET]\n",
    "with open(curated / out_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(selected, f, ensure_ascii=False)\n",
    "\n",
    "# (optional) tiny sample_submission to appease some loaders\n",
    "if RUN_SET == \"test\":\n",
    "    (curated / \"sample_submission.json\").write_text(\"{}\", encoding=\"utf-8\")\n",
    "\n",
    "# Force both env + local var to the curated directory\n",
    "os.environ[\"ARC_DATA_DIR\"] = str(curated)\n",
    "dataset_dir = curated\n",
    "print(f\"[CURATE] ARC_DATA_DIR -> {dataset_dir} (contains only {out_name})\")\n",
    "\n",
    "# === Critical: override the imported loader symbol so it ONLY returns 'selected' ===\n",
    "# If you've already done: from predict_competition import load_aggregated_tasks\n",
    "# we now rebind that local name here.\n",
    "try:\n",
    "    # If the symbol exists in globals, keep a reference (for debugging)\n",
    "    _orig_load_aggregated_tasks = globals().get(\"load_aggregated_tasks\", None)\n",
    "except Exception:\n",
    "    _orig_load_aggregated_tasks = None\n",
    "\n",
    "def load_aggregated_tasks(root: Path):\n",
    "    \"\"\"Notebook-level override: ignore what's on disk; use curated 'selected' only.\"\"\"\n",
    "    # safety: ensure the caller passed the curated path\n",
    "    root = Path(root)\n",
    "    if root.resolve() != dataset_dir.resolve():\n",
    "        print(f\"[OVERRIDE] Forcing dataset_dir from {root} to {dataset_dir}\")\n",
    "    return dict(selected)  # shallow copy\n",
    "\n",
    "print(\"[PATCH] load_aggregated_tasks() overridden to return curated 'selected' only.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ae4588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T02:59:42.052810Z",
     "iopub.status.busy": "2025-10-28T02:59:42.051842Z",
     "iopub.status.idle": "2025-10-28T02:59:42.058766Z",
     "shell.execute_reply": "2025-10-28T02:59:42.057541Z"
    },
    "papermill": {
     "duration": 0.012331,
     "end_time": "2025-10-28T02:59:42.060311",
     "exception": false,
     "start_time": "2025-10-28T02:59:42.047980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# QUIET MODE: suppress side outputs & traces before running the solver\n",
    "import os\n",
    "\n",
    "# hard off for any event/trace/dump paths your package might honor\n",
    "os.environ[\"ENABLE_EVENTS\"] = \"0\"\n",
    "os.environ[\"EVENT_VERBOSE\"] = \"0\"\n",
    "os.environ[\"ARC_ADAPTER_LOG\"] = \"0\"\n",
    "os.environ[\"ARC_DUMP_EXT_FIRST\"] = \"0\"\n",
    "os.environ[\"ARC_DEBUG_SCORES\"] = \"0\"\n",
    "os.environ[\"RIL_ENABLE_ENHANCEMENTS\"] = os.environ.get(\"RIL_ENABLE_ENHANCEMENTS\",\"1\")  # keep features\n",
    "os.environ[\"RIL_EMIT_TRACES\"] = \"0\"\n",
    "os.environ[\"RIL_TRACE_DIR\"] = \"/dev/null\"\n",
    "os.environ[\"RIL_DUMP_DIR\"] = \"/dev/null\"\n",
    "os.environ[\"ARC_USE_EXTERNAL\"] = os.environ.get(\"ARC_USE_EXTERNAL\",\"0\")  # keep your choice\n",
    "os.environ[\"ARC_EVAL_OK\"] = \"1\"\n",
    "os.environ[\"ARC_DISABLE_READINESS\"] = \"1\"  # keep readiness off in Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52a8b678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T02:59:42.066781Z",
     "iopub.status.busy": "2025-10-28T02:59:42.066468Z",
     "iopub.status.idle": "2025-10-28T03:00:18.322435Z",
     "shell.execute_reply": "2025-10-28T03:00:18.321358Z"
    },
    "papermill": {
     "duration": 36.263145,
     "end_time": "2025-10-28T03:00:18.325916",
     "exception": false,
     "start_time": "2025-10-28T02:59:42.062771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸  Set default values for: COMP_MODE, EVAL_MODE, NO_ADAPT, MODEL_FREEZE, DETERMINISTIC, RIL_SEED, RIL_DEVICE, ARC_ROUTER_POLICY\n",
      "âœ… Environment configured\n",
      "ðŸ’» Using CPU\n",
      "âœ… Loaded settings from /kaggle/working/arc_agi_2_submission/SETTINGS.json\n",
      "[TASKS] Will process 240 tasks (RUN_SET=test)\n",
      "[DEVICE] cpu\n",
      "[SUBMISSION]\n",
      "  file        : /kaggle/working/submission.json\n",
      "  tasks       : 240 (expected 240)\n",
      "  attempts    : top-2\n",
      "  policy      : hybrid_adapters_first\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Run solver on curated set and write Kaggle-compliant submission.json ===\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "import numpy as np\n",
    "\n",
    "# Safety: ensure we only run on the curated set created in Cell 2\n",
    "assert 'dataset_dir' in globals() and Path(dataset_dir).exists(), \"dataset_dir missing; run Cell 1 & 2 first.\"\n",
    "assert 'selected' in globals() and isinstance(selected, dict) and selected, \"'selected' tasks missing; run Cell 2 first.\"\n",
    "assert 'RUN_SET' in globals(), \"RUN_SET missing; run Cell 2 first.\"\n",
    "if RUN_SET != \"test\":\n",
    "    raise SystemExit(\"Submission must use RUN_SET='test'. Set ARC_RUN_SET=test and re-run Cell 2.\")\n",
    "\n",
    "# --- Environment hygiene ---\n",
    "os.environ.setdefault(\"ARC_EVAL_OK\", \"1\")\n",
    "os.environ.setdefault(\"ARC_DISABLE_READINESS\", \"1\")   # keep readiness OFF in Kaggle\n",
    "os.environ.setdefault(\"ARC_SUBMISSION_MODE\", \"1\")\n",
    "os.environ.setdefault(\"PYTHONHASHSEED\", \"1234\")\n",
    "\n",
    "# --- Import AFTER Cell 2 so our overridden load_aggregated_tasks stays in effect ---\n",
    "from predict_competition import (\n",
    "    load_settings,\n",
    "    setup_device,\n",
    "    validate_environment,\n",
    "    run_ril_prediction,\n",
    ")\n",
    "\n",
    "# --- Initialize runtime ---\n",
    "validate_environment()\n",
    "device = setup_device()\n",
    "settings = load_settings(\"SETTINGS.json\")\n",
    "if not settings:\n",
    "    raise SystemExit(\"SETTINGS.json could not be loaded; aborting.\")\n",
    "\n",
    "# --- Use ONLY curated tasks (local override from Cell 2) ---\n",
    "task_ids = sorted(selected.keys())\n",
    "print(f\"[TASKS] Will process {len(task_ids)} tasks (RUN_SET={RUN_SET})\")\n",
    "print(f\"[DEVICE] {device}\")\n",
    "\n",
    "# --- Filter solver METRICS noise printed to stderr/stdout in notebooks ---\n",
    "import io, sys, re, contextlib\n",
    "\n",
    "class _DropLines(io.TextIOBase):\n",
    "    \"\"\"Redirect stream that drops lines beginning with certain prefixes.\"\"\"\n",
    "    def __init__(self, orig_stream, prefixes=(\"[METRICS] \",)):\n",
    "        self._orig = orig_stream\n",
    "        self._buf = \"\"\n",
    "        self._pat = re.compile(rf\"^({'|'.join(re.escape(p) for p in prefixes)})\")\n",
    "\n",
    "    def write(self, s):\n",
    "        self._buf += s\n",
    "        out = []\n",
    "        while \"\\n\" in self._buf:\n",
    "            line, self._buf = self._buf.split(\"\\n\", 1)\n",
    "            if not self._pat.match(line):\n",
    "                out.append(line + \"\\n\")\n",
    "        if out:\n",
    "            self._orig.write(\"\".join(out))\n",
    "        return len(s)\n",
    "\n",
    "    def flush(self):\n",
    "        if self._buf:\n",
    "            if not self._pat.match(self._buf):\n",
    "                self._orig.write(self._buf)\n",
    "            self._buf = \"\"\n",
    "        self._orig.flush()\n",
    "\n",
    "TOPK = 2\n",
    "\n",
    "# --- Fully quiet run_ril_prediction: hide [METRICS] noise and low-level prints ---\n",
    "from IPython.utils import io as ipio\n",
    "import os, sys, contextlib\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _hard_mute_fds():\n",
    "    \"\"\"Silence OS-level writes to stdout/stderr during the solver call.\"\"\"\n",
    "    devnull = os.open(os.devnull, os.O_WRONLY)\n",
    "    old1, old2 = os.dup(1), os.dup(2)\n",
    "    try:\n",
    "        os.dup2(devnull, 1)\n",
    "        os.dup2(devnull, 2)\n",
    "        yield\n",
    "    finally:\n",
    "        os.dup2(old1, 1)\n",
    "        os.dup2(old2, 2)\n",
    "        os.close(old1)\n",
    "        os.close(old2)\n",
    "        os.close(devnull)\n",
    "\n",
    "# Layered suppression:\n",
    "#   1. Filter Python stdout/stderr lines starting with [METRICS]\n",
    "#   2. Capture Jupyter display output\n",
    "#   3. Mute OS-level file descriptors\n",
    "with contextlib.redirect_stderr(_DropLines(sys.stderr, prefixes=(\"[METRICS] \",))):\n",
    "    with contextlib.redirect_stdout(_DropLines(sys.stdout, prefixes=(\"[METRICS] \",))):\n",
    "        with ipio.capture_output() as _:\n",
    "            with _hard_mute_fds():\n",
    "                predictions, policy_name, total_cases = run_ril_prediction(\n",
    "                    task_ids, settings, topk=TOPK\n",
    "                )\n",
    "\n",
    "# ------------------------ STRICT WRITER (no sample dependency) ------------------------\n",
    "# We read case counts from the curated test file in dataset_dir and emit dict-of-attempts.\n",
    "def _find_test_file(curated_dir: Path) -> Path | None:\n",
    "    for name in (\"arc-agi_test_challenges.json\", \"arc-agi_test-challenges.json\"):\n",
    "        p = curated_dir / name\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "test_file = _find_test_file(Path(dataset_dir))\n",
    "if not test_file:\n",
    "    raise SystemExit(f\"No test challenges JSON found in curated dir: {dataset_dir}\")\n",
    "\n",
    "with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_blob = json.load(f)\n",
    "\n",
    "def _normalize_test_cases(blob):\n",
    "    out = {}\n",
    "    if isinstance(blob, dict):\n",
    "        for tid, task in blob.items():\n",
    "            tests = task.get(\"test\") or []\n",
    "            out[str(tid)] = int(len(tests))\n",
    "    else:\n",
    "        for item in blob:\n",
    "            if not isinstance(item, dict): \n",
    "                continue\n",
    "            tid = item.get(\"task_id\") or item.get(\"id\")\n",
    "            task = item.get(\"task\") or item\n",
    "            if tid and isinstance(task, dict):\n",
    "                tests = task.get(\"test\") or []\n",
    "                out[str(tid)] = int(len(tests))\n",
    "    return out\n",
    "\n",
    "test_cases = _normalize_test_cases(test_blob)\n",
    "test_tids = sorted(test_cases.keys())\n",
    "\n",
    "def _to_grid_safe(grid):\n",
    "    arr = np.asarray(grid, dtype=int)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr[None, :]\n",
    "    if arr.ndim != 2 or arr.size == 0:\n",
    "        return [[0]]\n",
    "    if arr.shape[0] > 30 or arr.shape[1] > 30:\n",
    "        arr = arr[:30, :30]\n",
    "    arr = np.clip(arr, 0, 9).astype(int)\n",
    "    return arr.tolist()\n",
    "\n",
    "def _attempts_from(pred_entry):\n",
    "    if isinstance(pred_entry, dict):\n",
    "        items = sorted(\n",
    "            pred_entry.items(),\n",
    "            key=lambda kv: (int(str(kv[0]).split(\"_\")[-1]) if str(kv[0]).split(\"_\")[-1].isdigit() else 999, str(kv[0]))\n",
    "        )\n",
    "        seq = [kv[1] for kv in items]\n",
    "    elif isinstance(pred_entry, list):\n",
    "        seq = pred_entry\n",
    "    else:\n",
    "        seq = []\n",
    "    return [_to_grid_safe(g) for g in seq] or [[[0]]]\n",
    "\n",
    "TOPK = int(TOPK) if isinstance(TOPK, (int, np.integer)) else 2\n",
    "submission_out = {}\n",
    "missing_tasks = 0\n",
    "\n",
    "for tid in test_tids:\n",
    "    n_cases = test_cases[tid]\n",
    "    pred_task = predictions.get(tid, [])\n",
    "    if isinstance(pred_task, dict):\n",
    "        pred_cases = [pred_task[k] for k in sorted(pred_task.keys(), key=lambda x: int(x) if str(x).isdigit() else 1_000_000)]\n",
    "    elif isinstance(pred_task, list):\n",
    "        pred_cases = pred_task\n",
    "    else:\n",
    "        pred_cases = []\n",
    "\n",
    "    if not pred_cases:\n",
    "        missing_tasks += 1\n",
    "\n",
    "    case_objs = []\n",
    "    for ci in range(n_cases):\n",
    "        case_pred = pred_cases[ci] if ci < len(pred_cases) else []\n",
    "        attempts = _attempts_from(case_pred)\n",
    "        # enforce exactly TOPK attempts by truncating/padding\n",
    "        if len(attempts) > TOPK:\n",
    "            attempts = attempts[:TOPK]\n",
    "        elif len(attempts) < TOPK:\n",
    "            attempts += [attempts[-1]] * (TOPK - len(attempts))\n",
    "        case_objs.append({f\"attempt_{i+1}\": g for i, g in enumerate(attempts)})\n",
    "\n",
    "    submission_out[tid] = case_objs\n",
    "\n",
    "with open(\"submission.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(submission_out, f, ensure_ascii=False)\n",
    "\n",
    "print(\"[SUBMISSION]\")\n",
    "print(f\"  file        : {Path('submission.json').resolve()}\")\n",
    "print(f\"  tasks       : {len(submission_out)} (expected {len(test_tids)})\")\n",
    "print(f\"  attempts    : top-{TOPK}\")\n",
    "print(f\"  policy      : {policy_name}\")\n",
    "if missing_tasks:\n",
    "    print(f\"[WARN] {missing_tasks} tasks had no predictions; filled with [[0]] placeholders.\")\n",
    "# --------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d620eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T03:00:18.333295Z",
     "iopub.status.busy": "2025-10-28T03:00:18.332864Z",
     "iopub.status.idle": "2025-10-28T03:00:18.347873Z",
     "shell.execute_reply": "2025-10-28T03:00:18.346843Z"
    },
    "papermill": {
     "duration": 0.020744,
     "end_time": "2025-10-28T03:00:18.349496",
     "exception": false,
     "start_time": "2025-10-28T03:00:18.328752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLEANUP] Remaining files:\n",
      "  - submission.json\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP: leave only submission.json in /kaggle/working\n",
    "from pathlib import Path\n",
    "import shutil, os\n",
    "\n",
    "keep = {\"submission.json\"}\n",
    "root = Path(\"/kaggle/working\")\n",
    "\n",
    "def _safe_rm(p: Path):\n",
    "    try:\n",
    "        if p.is_dir():\n",
    "            shutil.rmtree(p, ignore_errors=True)\n",
    "        else:\n",
    "            p.unlink(missing_ok=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# remove our curated dataset if it exists\n",
    "curated = root / \"arc_curated_dataset\"\n",
    "if curated.exists():\n",
    "    _safe_rm(curated)\n",
    "\n",
    "# remove copied package (optional; not needed by scorer)\n",
    "pkg = root / \"arc_agi_2_submission\"\n",
    "if pkg.exists():\n",
    "    _safe_rm(pkg)\n",
    "\n",
    "# remove everything else except submission.json\n",
    "for p in root.iterdir():\n",
    "    if p.name in keep:\n",
    "        continue\n",
    "    # ignore Kaggle system dirs that shouldn't be touched\n",
    "    if p.name in {\".ipynb_checkpoints\"}:\n",
    "        continue\n",
    "    _safe_rm(p)\n",
    "\n",
    "# final confirmation\n",
    "print(\"[CLEANUP] Remaining files:\")\n",
    "for p in sorted(root.iterdir()):\n",
    "    print(\"  -\", p.name)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 8588334,
     "sourceId": 13525907,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 44.099445,
   "end_time": "2025-10-28T03:00:19.977133",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-28T02:59:35.877688",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
